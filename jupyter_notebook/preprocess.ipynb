{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import MeCab\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_FILES_DIRECTORY = '../data/wikipedia/*/*'\n",
    "ABSOLUTE_PATH = Path().resolve()\n",
    "DB_FILE = 'corpus_data.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiProcessing():\n",
    "    ''' Wikipedia関連の処理を行うclass '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def get_wikifiles(self, wiki_files_directory):\n",
    "        return glob.glob(wiki_files_directory)\n",
    "\n",
    "    def separate_paragraph_array(self, document):\n",
    "        contents = document.read()\n",
    "        # docタグ以外のタグは削除\n",
    "        contents = re.sub(r'\\<((?!doc).)*?\\>', '', contents)\n",
    "        contents = re.sub(r'\\</((?!doc).)*?\\>', '', contents)\n",
    "        # BeautifulSoupの仕様?頭に何かタグを入れないと、<doc>タグを最初の一つしか読まない\n",
    "        contents = '<docs>\\n' + contents + '</docs>'\n",
    "        soup = BeautifulSoup(contents, \"xml\")\n",
    "        wiki_items = soup.find_all('doc')\n",
    "\n",
    "        wiki_text = ''\n",
    "        for wiki_item in wiki_items:\n",
    "            wiki_text += wiki_item.get_text()\n",
    "        return wiki_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbModel:\n",
    "    ''' Sqliteの処理を行うclass '''\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "        if not os.path.isfile(os.path.join(ABSOLUTE_PATH, db_file)):\n",
    "            self._init_process()\n",
    "            self._create_table()\n",
    "        else:\n",
    "            self._init_process()\n",
    "\n",
    "    def _init_process(self):\n",
    "        self.conn = sqlite3.connect(self.db_file)\n",
    "        self.cur = self.conn.cursor()\n",
    "\n",
    "    def _create_table(self):\n",
    "        self.cur.execute('''CREATE TABLE words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word STRING,\n",
    "            frequency INTEGER\n",
    "        )''')\n",
    "        self.cur.execute('''CREATE TABLE sentences (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            file_name STRING,\n",
    "            word_sentence STRING,\n",
    "            index_sentence STRING\n",
    "        )''')\n",
    "        return self.conn, self.cur\n",
    "\n",
    "    def select_all_records_words_table(self):\n",
    "        select_word_sql = 'SELECT * FROM words'\n",
    "        self.cur.execute(select_word_sql)\n",
    "        return self.cur.fetchall()\n",
    "\n",
    "    def insert_records_words_table(self, words_frequency):\n",
    "        insert_word_sql = 'INSERT INTO words (word, frequency) values (?,?)'\n",
    "        inserted_info = []\n",
    "        for word, frequency in dict(words_frequency).items():\n",
    "            inserted_info.append((word, frequency))\n",
    "        self.cur.executemany(insert_word_sql, inserted_info)\n",
    "\n",
    "    def insert_records_sentences_table(self, inserted_info):\n",
    "        insert_sentence_sql = '''\n",
    "            INSERT INTO sentences (\n",
    "                file_name, word_sentence, index_sentence\n",
    "            ) values (?,?,?)\n",
    "        '''\n",
    "        self.cur.executemany(insert_sentence_sql, inserted_info)\n",
    "\n",
    "    def close_connection(self):\n",
    "        self.conn.commit()\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordProcessing():\n",
    "    ''' 文章から単語の出現頻度を求めたり、単語をindexにしたりするclass '''\n",
    "    def __init__(self, corpus, docs_files):\n",
    "        self.corpus = corpus\n",
    "        self.docs_files = docs_files\n",
    "        self.file_num = len(docs_files)\n",
    "        # 日本語の表層形でなく、基本形を使用するために、パーサーとしてOchasenを使用\n",
    "        self.tagger = MeCab.Tagger('-Ochasen')\n",
    "\n",
    "    def extract_words_fequency(self):\n",
    "        words_frequency = Counter({})\n",
    "        unnecessary_words = []\n",
    "\n",
    "        pbar = tqdm(total=self.file_num)\n",
    "        pbar.set_description('extract words fequency')\n",
    "        for docs_file in self.docs_files:\n",
    "            pbar.update(1)\n",
    "            with open(docs_file) as doc:\n",
    "                paragraph_sentence_list =\\\n",
    "                    self.corpus.separate_paragraph_array(doc)\n",
    "                words_list = []\n",
    "                for paragraph_sentence in paragraph_sentence_list:\n",
    "                    # サンプル文を形態素解析した単語群にパース\n",
    "                    node = self.tagger.parseToNode(paragraph_sentence)\n",
    "                    paragraph_words = []\n",
    "                    while node:\n",
    "                        #  node.featureで、該当単語に対して以下の特徴抽出ができる\n",
    "                        # [品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音]\n",
    "                        # 表層形でなく、基本形(原形)を採用\n",
    "                        word = node.feature.split(\",\")[6]\n",
    "                        # 日本語の表現だけを抽出\n",
    "                        if re.search(r'[ぁ-んァ-ヶ一-龥]+', word):\n",
    "                            paragraph_words.append(word)\n",
    "                        else:\n",
    "                            if word not in unnecessary_words:\n",
    "                                unnecessary_words.append(word)\n",
    "                        node = node.next\n",
    "                    words_list.extend(paragraph_words)\n",
    "\n",
    "                words_frequency += Counter(words_list)\n",
    "        pbar.close()\n",
    "        return words_frequency\n",
    "\n",
    "    def _create_word2idx_dict(self, words_info):\n",
    "        word_stoi = {}\n",
    "        for word_info in words_info:\n",
    "            word_stoi[word_info[1]] = word_info[0]\n",
    "        return word_stoi\n",
    "\n",
    "    def transfer_sentence_word2idx(self, words_info, db_model):\n",
    "        word_stoi = self._create_word2idx_dict(words_info)\n",
    "\n",
    "        pbar = tqdm(total=self.file_num)\n",
    "        pbar.set_description('transfer sentence word2idx')\n",
    "        for wiki_file in self.docs_files:\n",
    "            pbar.update(1)\n",
    "            with open(wiki_file) as doc:\n",
    "                inserted_info = []\n",
    "\n",
    "                paragraph_sentence_list =\\\n",
    "                    self.corpus.separate_paragraph_array(doc)\n",
    "                for paragraph_sentence in paragraph_sentence_list:\n",
    "                    # サンプル文を形態素解析した単語群にパース\n",
    "                    node = self.tagger.parseToNode(paragraph_sentence)\n",
    "                    paragraph_words = ''\n",
    "                    while node:\n",
    "                        #  node.featureで、該当単語に対して以下の特徴抽出ができる\n",
    "                        # [品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音]\n",
    "                        # 表層形でなく、基本形(原形)を採用\n",
    "                        word = node.feature.split(\",\")[6]\n",
    "                        # 日本語の表現だけを抽出\n",
    "                        if re.search(r'[ぁ-んァ-ヶ一-龥]+', word):\n",
    "                            paragraph_words += str(word_stoi[word]) + ', '\n",
    "                        node = node.next\n",
    "\n",
    "                    if not paragraph_words:\n",
    "                        inserted_info.append((\n",
    "                            wiki_file, paragraph_sentence, paragraph_words\n",
    "                        ))\n",
    "\n",
    "                # 文章情報のDB登録\n",
    "                db_model.insert_records_sentences_table(inserted_info)\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract words fequency: 100%|██████████| 100/100 [00:32<00:00,  2.53it/s]\n",
      "transfer sentence word2idx: 100%|██████████| 100/100 [00:32<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiProcessing()\n",
    "\n",
    "# wikipediaのファイルpath一覧を取得\n",
    "wiki_files = wiki.get_wikifiles(WIKI_FILES_DIRECTORY)\n",
    "\n",
    "# wikipedia上に出現する単語とその出現頻度を算出\n",
    "wiki_word_processing = WordProcessing(wiki, wiki_files)\n",
    "wiki_words_frequency = wiki_word_processing.extract_words_fequency()\n",
    "\n",
    "# 単語とその出現頻度をDB格納\n",
    "db_model = DbModel(DB_FILE)\n",
    "db_model.insert_records_words_table(wiki_words_frequency)\n",
    "\n",
    "# テキストデータを単語インデックスの配列情報としてDB格納\n",
    "words_info = db_model.select_all_records_words_table()\n",
    "wiki_word_processing.transfer_sentence_word2idx(words_info, db_model)\n",
    "\n",
    "db_model.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
